{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zAdgNNhm0b1l"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    # constructor (__init__)\n",
        "    # initializes neural network's architecture\n",
        "    # input_size: size of input features\n",
        "    # hidden_size: no. of neurons in hidden layer\n",
        "    # num_classes: no. of output classes\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Layers defined using nn.Linear\n",
        "        # connects input neurons to hidden\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        # connects hidden to subsequent hidden neurons\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        # connects hidden to output\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        # ReLU: Rectified Linear Unit\n",
        "        # used to introduce non-linearity in network\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    # defines forward pass of neural network\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Fyg1WT0uZL",
        "outputId": "6485c10c-ec08-4a71-a563-c8a465b51465"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# nltk_utils\n",
        "import numpy as np\n",
        "import nltk\n",
        "# natural language toolkit\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# splits sentence to array of words or tokens\n",
        "\n",
        "# uses NLTKs word_tokenize method for splitting\n",
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    split sentence into array of words/tokens\n",
        "    a token can be a word or punctuation character, or number\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    stemming = find the root form of the word\n",
        "    examples:\n",
        "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
        "    words = [stem(w) for w in words]\n",
        "    -> [\"organ\", \"organ\", \"organ\"]\n",
        "    \"\"\"\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "\n",
        "def bag_of_words(tokenized_sentence, words):\n",
        "    \"\"\"\n",
        "    return bag of words array:\n",
        "    1 for each known word that exists in the sentence, 0 otherwise\n",
        "    example:\n",
        "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
        "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
        "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
        "    \"\"\"\n",
        "    # stem each word\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    # initialize bag with 0 for each word\n",
        "    bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words:\n",
        "            bag[idx] = 1\n",
        "\n",
        "    return bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKYCA6X5bHZ3",
        "outputId": "27845b3c-dec9-44d7-994e-7a1920fd0532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "259 patterns\n",
            "26 tags: ['admission', 'campus area', 'canteen', 'committee', 'course', 'document', 'event', 'facilities', 'fees', 'goodbye', 'greeting', 'hostel', 'hours', 'infrastructure', 'library', 'location', 'number', 'placement', 'principal', 'ragging', 'ranking', 'scholarship', 'sem', 'sports', 'uniform', 'vacation']\n",
            "178 unique stemmed words: [\"'s\", 'a', 'about', 'ac', 'activ', 'address', 'admis', 'admiss', 'against', 'ai/ml', 'an', 'and', 'ani', 'antirag', 'are', 'area', 'at', 'automobil', 'avail', 'averag', 'be', 'between', 'big', 'book', 'boy', 'branch', 'bring', 'build', 'bye', 'cafetaria', 'campu', 'can', 'canteen', 'capac', 'case', 'casual', 'ce', 'chemic', 'civil', 'code', 'colleg', 'committ', 'committe', 'comp', 'compani', 'comput', 'conduct', 'contact', 'cours', 'date', 'day', 'detail', 'differ', 'distanc', 'do', 'document', 'doe', 'done', 'dress', 'dresscod', 'dure', 'each', 'end', 'engin', 'event', 'exam', 'facil', 'far', 'fee', 'first', 'food', 'for', 'fourth', 'from', 'function', 'game', 'get', 'girl', 'give', 'goodby', 'guy', 'have', 'held', 'hello', 'here', 'heyi', 'hi', 'histori', 'holiday', 'hostel', 'hour', 'how', 'i', 'in', 'incid', 'info', 'inform', 'infrastructur', 'is', 'it', 'junior', 'librari', 'list', 'locat', 'long', 'mani', 'me', 'mechan', 'much', 'my', 'name', 'need', 'no', 'non-ac', 'number', 'of', 'offer', 'offic', 'on', 'open', 'oper', 'organis', 'overal', 'packag', 'per', 'phone', 'placement', 'pleas', 'practic', 'princip', 'process', 'provid', 'rag', 'rank', 'rate', 'recruit', 'requir', 'room', 'saturday', 'schedul', 'scholarship', 'second', 'sem', 'semest', 'serviv', 'size', 'someth', 'sport', 'start', 'take', 'technolog', 'telephon', 'tell', 'thapar', 'the', 'there', 'these', 'third', 'time', 'timet', 'to', 'uniform', 'vacat', 'visit', 'we', 'wear', 'what', 'whatv', 'when', 'where', 'which', 'who', 'will', 'work', 'world', 'year', 'you', 'your']\n",
            "178 26\n",
            "Epoch [100/1000], Loss: 0.0050\n",
            "Epoch [200/1000], Loss: 0.0023\n",
            "Epoch [300/1000], Loss: 0.0007\n",
            "Epoch [400/1000], Loss: 0.0001\n",
            "Epoch [500/1000], Loss: 0.0000\n",
            "Epoch [600/1000], Loss: 0.0000\n",
            "Epoch [700/1000], Loss: 0.0000\n",
            "Epoch [800/1000], Loss: 0.0000\n",
            "Epoch [900/1000], Loss: 0.0000\n",
            "Epoch [1000/1000], Loss: 0.0000\n",
            "final loss: 0.0000\n",
            "training complete. file saved to data.pth\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "with open('intents.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    # add to tag list\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = tokenize(pattern)\n",
        "        # add to our words list\n",
        "        all_words.extend(w)\n",
        "        # add to xy pair\n",
        "        xy.append((w, tag))\n",
        "\n",
        "# stem and lower each word\n",
        "ignore_words = ['?', '.', '!']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "# remove duplicates and sort\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)\n",
        "\n",
        "# create training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    # X: bag of words for each pattern_sentence\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    X_train.append(bag)\n",
        "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "input_size = len(X_train[0])\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "print(input_size, output_size)\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words)\n",
        "        # if y would be one-hot, we must apply\n",
        "        # labels = torch.max(labels, 1)[1]\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')\n",
        "\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": input_size,\n",
        "\"hidden_size\": hidden_size,\n",
        "\"output_size\": output_size,\n",
        "\"all_words\": all_words,\n",
        "\"tags\": tags\n",
        "}\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0tWleod0139",
        "outputId": "e4b5f602-6eb1-44cd-891f-0b89036aede8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! \n",
            "Type 'quit' to exit\n",
            "ThaparBot: You can contact at: 0175 239 3021\n",
            "ThaparBot: Our university offers: \n",
            "1. Chemical Engineering, \n",
            "2. Civil Engineering, \n",
            "3. Computer Engineering, \n",
            "4. Electrical Engineering, \n",
            "5. Electronics and Communication Engineering, \n",
            "6. Electronics Instrumentation and Control Engineering, \n",
            "7. Electronics and Computer Engineering, \n",
            "8. Mechanical Engineering, \n",
            "9. Mechatronics, \n",
            "10. Biotechnology.\n",
            "ThaparBot: University fees per semester is around 1.5-2 lacs.And hostel fees ranges between Rs.40500-70000 depending upon the type of accomodation.\n",
            "ThaparBot: Our University has Excellent Infrastructure. Campus is clean. Good IT Labs With Good Speed of Internet connection. You may also visit https://tiet360.in/ for campus tour.\n",
            "ThaparBot: Our university's Engineering department provides fully AC Lab with internet connection, smart classroom, Auditorium, library,canteen\n",
            "ThaparBot: Our university's Engineering department provides fully AC Lab with internet connection, smart classroom, Auditorium, library,canteen\n",
            "ThaparBot: College organises various events every semester. Some of the famous events are Saturnalia, izhaar, tff, etc.\n",
            "ThaparBot: Our university's Engineering department provides fully AC Lab with internet connection, smart classroom, Auditorium, library,canteen\n"
          ]
        }
      ],
      "source": [
        "# chat\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load contents\n",
        "with open('E:\\All semesters\\Projects\\8. Thapar Query Chatbot ML\\ML_Project_Chatbot\\ML_Project_Chatbot\\intents.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "# Load preprocessed data and model from a saved file\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "# Extraction of components from loaded data\n",
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "all_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "bot_name = \"ThaparBot\"\n",
        "print(\"Let's chat! \\nType 'quit' to exit\")\n",
        "while True:\n",
        "    sentence = input(\"You: \")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "\n",
        "    # tokenize user input\n",
        "    sentence = tokenize(sentence)\n",
        "    # convert user input to bag of words representation\n",
        "    X = bag_of_words(sentence, all_words)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "    # make prediction with trained model\n",
        "    output = model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = tags[predicted.item()]\n",
        "\n",
        "    # calculate prob and check confidence level\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents['intents']:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "    else:\n",
        "        print(f\"{bot_name}: Sorry, I do not understand.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
